{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    '''\n",
    "    training loop for a single epoch including attribution prior\n",
    "    output --> average loss on the training dataset including attribution loss\n",
    "    '''\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    test_accuracy = []\n",
    "    attribution_loss = []\n",
    "    classif_loss = []\n",
    "    \n",
    "    count = 0\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        _, sparse_labels = y_hat.max(dim=1)\n",
    "        \n",
    "        # input x = (64, 1, 240), sparse_labels = [0,1,1,0,0] --> output (64, 1, 240)\n",
    "        # attributions are calculated using Expected gradients and a single background reference\n",
    "        attributions = APExp.shap_values(model, x, sparse_labels=sparse_labels)\n",
    "        \n",
    "        # standardize each attribution based on its var and mean --> output (64, 1, 240)\n",
    "        normalized_attributions = per_x_standardization(attributions)\n",
    "        \n",
    "        # pass attributions to a differentiable regularization function \"pix_loss\" that encourages smooth attributions \n",
    "        var = pix_loss(normalized_attributions, tv_weight=tv_weight) \n",
    "        attribution_prior = var[~torch.isnan(var)].mean() # take mean to return penalizing scalar (torch.nanmean broken)\n",
    "        \n",
    "        # crossentropy loss for prediction\n",
    "        classification_loss = criterion(y_hat, y)\n",
    "        \n",
    "        # total loss includes cross entropy plus the loss from the attribution prior regularized by lambda\n",
    "        total_loss = classification_loss + lamb * attribution_prior\n",
    "        \n",
    "        total_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect losses\n",
    "        train_loss.append( total_loss.item() )\n",
    "        attribution_loss.append( (lamb * attribution_prior).item() )\n",
    "        classif_loss.append( classification_loss.item() )\n",
    "        \n",
    "        # break epoch early so there is enough data and program doesnâ€™t crash\n",
    "        if count == len(train_dataloader) - 10:\n",
    "            break\n",
    "        count += 1\n",
    "    \n",
    "    return train_loss, classif_loss, attribution_loss\n",
    "\n",
    "\n",
    "def test():\n",
    "    '''\n",
    "    Test loop for the model\n",
    "    output --> average loss on the test dataset after each epoch  \n",
    "    '''\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            val_loss.append(loss.item())\n",
    "        val_loss = np.array(val_loss)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def per_x_standardization(x):\n",
    "    '''\n",
    "    Standardizes each spectras attributions\n",
    "    Equivalent behavior of tf.image.per_image_standardization but for 1D\n",
    "    \n",
    "    Input:\n",
    "          x --> (-1, 1, features) tensor\n",
    "    Output:\n",
    "          normalized_attributions --> (-1, 1, features) \n",
    "    '''\n",
    "    mean = torch.mean(x, 2)\n",
    "    # provide an alternative lower limit so no divide by zero\n",
    "    cut = torch.Tensor([1.0/x.shape[-1]])\n",
    "    cut = cut.expand(x.shape[0], 1).to(device)\n",
    "    adjusted_stddev = torch.max(torch.std(x, 2, unbiased=True), cut)\n",
    "    normalized_attributions = (np.squeeze(x) - mean) / adjusted_stddev\n",
    "    \n",
    "    return normalized_attributions.unsqueeze(1)\n",
    "\n",
    "\n",
    "def pix_loss(nprof, tv_weight=5):\n",
    "    \"\"\"\n",
    "    Compute total variational across neighboring pixels\n",
    "    Replicates functionality of tf.image.total_variation but for 1D\n",
    "    \n",
    "    Inputs:\n",
    "    - nprof: Tensor of shape (batch, 1, features)\n",
    "    - tv_weight: Scalar giving the weight w_t to use for the TV loss. For Expected grad tv_weight = 1\n",
    "    Output:\n",
    "    - loss: Tensor holding a vector giving the total variation loss for each spectrum\n",
    "    \"\"\"\n",
    "    variance = torch.sum(torch.abs(nprof[:,:,:-1] - nprof[:,:,1:]), axis=2)\n",
    "    loss = tv_weight * variance\n",
    "    \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
