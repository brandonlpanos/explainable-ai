{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib as mpl\n",
    "from scipy import interpolate\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from captum.attr import GradientShap\n",
    "from scipy.signal import savgol_filter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import *\n",
    "from qs_vae import *\n",
    "from utils_cleaning import *\n",
    "from utils_attributions import *\n",
    "import warnings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.seterr(divide='ignore', invalid='ignore');\n",
    "warnings.filterwarnings(action='ignore', message='All-NaN slice encountered')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content  \n",
    "In this notebook, we use a subset of 9 models that best represent the attributions from the entire swarm of 50 models on a random sample of spectra. Reducing the number of models to 9 allows sue to make the attribution calculation for the entire pre-flare dataset tractable. \n",
    "\n",
    "Using these models we calculate the average attention for every spectrum in the entire PF dataset with respect to the ***Expected Gradients*** formalism. This represents a fast implementation with reduced models and number of background references. The full version with background references set to 500 and models to 50 should be run on key examples, but for now the results should be adequate. The notebooks return three matrices per observation \n",
    "- ***y_hats*** $\\to$ A dataset of shape (step, time, y) that contains the probability of each spectrum in the observation belonging to the PF-class  \n",
    "- ***y_raws*** $\\to$ A dataset of shape (step, time, y) that contains the raw output of the positive channel of the network before a Softmax activation function. The raw output allows us to absorb and reflect the high scores in the heatmaps color, which would otherwise be saturated at 1  \n",
    "- ***attributions*** $\\to$ A dataset of shape (step, time, y, lambda) that consists of attributions instead of spectra    \n",
    "\n",
    "The data structure is a one-to-one of the input observation, meaning all the attribution data for a particular spectrum can be found at the same location as the spectrum in the original dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Select ensemble model subset to use and average over***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 44 # model corresponding to a particular test/train split of the observations\n",
    "degree_of_smooth = 41 # post-hoc smooth of attributions\n",
    "baseline_samples = 500 # number of background data used to calculate attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect paths to models\n",
    "path_to_models = '/data1/userspace/bpanos/XAI/models/EG/'\n",
    "model_paths = [ f'{path_to_models}{name}' for name in os.listdir(path_to_models) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Construct background dataset***  \n",
    "Attributions in Expected Gradients are calculated by comparing the output of the network when different backgrounds are joined to the original spectra with varying strengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/data1/userspace/bpanos/XAI/models/ConvNet/{fold}.p', 'rb') as f: dic = pickle.load(f)\n",
    "path_to_clean_data = '/data1/userspace/bpanos/XAI/data/ConvNet/clean_strong/'\n",
    "train_obs = dic['train_obs']\n",
    "AR_train = []\n",
    "PF_train = []\n",
    "for n in train_obs:\n",
    "    if 'AR' in n:\n",
    "        AR_train.append(n)\n",
    "    if 'PF' in n:\n",
    "        PF_train.append(n)\n",
    "X_train_AR = None\n",
    "X_train_PF = None\n",
    "for AR_file, PF_file in zip(AR_train, PF_train):\n",
    "    fhand = np.load(path_to_clean_data + AR_file)\n",
    "    X_AR = fhand['X']\n",
    "    try: X_train_AR = np.concatenate( (X_train_AR, X_AR), axis=0 )\n",
    "    except: X_train_AR = X_AR\n",
    "    fhand = np.load(path_to_clean_data + PF_file)\n",
    "    X_PF = fhand['X']\n",
    "    try: X_train_PF = np.concatenate( (X_train_PF, X_PF), axis=0 )\n",
    "    except: X_train_PF = X_PF\n",
    "X_train_AR, X_train_PF = Balance(X_train_AR, X_train_PF)\n",
    "y_train_AR = np.zeros(len(X_train_AR))\n",
    "y_train_PF = np.ones(len(X_train_PF))\n",
    "X_train = np.concatenate((X_train_AR, X_train_PF), axis=0)\n",
    "y_train = np.concatenate((y_train_AR, y_train_PF))\n",
    "baseline = torch.Tensor(X_train).to(device).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Calculate the attributions of every spectra in every PF observation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to data and path to save attributions\n",
    "path_to_data = '/data1/iris_level_2C_10DNs/'\n",
    "path_to_save = '/data1/userspace/bpanos/XAI/data/attributions/EG/'\n",
    "\n",
    "# iterate over all PF observations\n",
    "for file in os.listdir(path_to_data):\n",
    "    if 'PF' not in file: continue # only compute attributions for PF obs\n",
    "    if file in os.listdir(path_to_save): continue\n",
    "    obs = path_to_data + file\n",
    "    print(obs)\n",
    "    # load data, these functions are more than required\n",
    "    nprof_orig, data = process_obs(vae_model, obs, mode='Raw')\n",
    "    _, nprof_clean = process_obs(vae_model, obs, mode='clean')\n",
    "    \n",
    "    # collect params of data shape\n",
    "    n_y = data.shape[2]\n",
    "    n_time = data.shape[1]\n",
    "    n_steps = data.shape[0]\n",
    "    n_lambda = data.shape[3]\n",
    "    \n",
    "    # initiate empty matrices to store the attributions and scores\n",
    "    y_raw_cube = np.zeros(data.shape[:-1], dtype='float32')\n",
    "    y_hat_cube = np.zeros(data.shape[:-1], dtype='float32')\n",
    "    attribution_cube = np.zeros_like(data, dtype='float32')\n",
    "    \n",
    "    # iterate over every spectra in the observation and fill in the empty matrices\n",
    "    for time in range(n_time):\n",
    "        for step in range(n_steps):\n",
    "            for y in range(n_y):\n",
    "                \n",
    "                spectrum = data[step, time, y, :]\n",
    "\n",
    "                # attributions of single spectrum are calculated here\n",
    "                y_raws = []\n",
    "                y_hats = []\n",
    "                attributions = []\n",
    "                for path in model_paths:\n",
    "                    model = CNN(num_classes=2)\n",
    "                    model.load_state_dict(torch.load(path))\n",
    "                    model.to(device)\n",
    "                    model.eval();\n",
    "                    y_raw, y_hat, attribution = EG_attributions(spectrum, baseline, model, n_samples=baseline_samples, degree_of_smooth=degree_of_smooth)\n",
    "                    y_raws.append(y_raw)\n",
    "                    y_hats.append(y_hat)\n",
    "                    attributions.append(attribution)\n",
    "                    del model\n",
    "\n",
    "                # avarage over all models\n",
    "                y_raw = np.nanmean(y_raws)\n",
    "                y_raw_cube[step, time, y] = y_raw\n",
    "\n",
    "                y_hat = np.nanmean(y_hats)\n",
    "                y_hat_cube[step, time, y] = y_hat\n",
    "\n",
    "                attributions = np.vstack(attributions)\n",
    "                attributions = np.nanmean(attributions, axis=0)\n",
    "                attribution_cube[step, time, y, :] = attributions\n",
    "    \n",
    "    # save matrices\n",
    "    np.savez( path_to_save + file[:-4], y_raws=y_raw_cube, y_hats=y_hat_cube, attributions=attribution_cube)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
